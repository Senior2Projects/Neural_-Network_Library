<<<<<<< Updated upstream
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4466ca5e",
   "metadata": {},
   "source": [
    "# Neural Network Library - Project Demo\n",
    "\n",
    "CSE473s: Computational Intelligence - Fall 2025\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete neural network library built from scratch using only NumPy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00eed47",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, we'll import the necessary libraries and our custom neural network implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c566689d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"f:\\University\\Senior_2\\CI\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\University\\Senior_2\\CI\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Import our neural network library\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Network, Dense, ReLU, Sigmoid, Tanh, Softmax, MSE, SGD, plot_losses, plot_decision_boundary\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\University\\Senior_2\\CI\\Project\\.venv\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\University\\Senior_2\\CI\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[39m\n\u001b[32m     86\u001b[39m     sys.setdlopenflags(_default_dlopen_flags)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     89\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback.format_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     90\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     91\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     92\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     93\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIf you need help, create an issue \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     94\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     95\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mand include the entire stack trace above this error message.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Traceback (most recent call last):\n  File \"f:\\University\\Senior_2\\CI\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "# Add library to path\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "# Import our neural network library\n",
    "from lib import Network, Dense, ReLU, Sigmoid, Tanh, Softmax, MSE, SGD, plot_losses, plot_decision_boundary\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d7fc7",
   "metadata": {},
   "source": [
    "## Part 1: XOR Problem Setup\n",
    "\n",
    "We'll start with the classic XOR dataset, which is a simple binary classification problem that requires a non-linear decision boundary. The inputs are bipolar (-1 or 1) and the target outputs follow the XOR logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X = np.array([\n",
    "    [-1,-1],\n",
    "    [-1,1],\n",
    "    [1,-1],\n",
    "    [1,1]\n",
    "], dtype=float)\n",
    "\n",
    "y = np.array([\n",
    "    [-1],\n",
    "    [1],\n",
    "    [1],\n",
    "    [-1]\n",
    "], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ad02b",
   "metadata": {},
   "source": [
    "### Training the Custom Neural Network\n",
    "\n",
    "Now we'll build and train our custom neural network. The architecture consists of:\n",
    "- Input layer: 2 neurons (for 2D inputs)\n",
    "- Hidden layer: 4 neurons with Tanh activation\n",
    "- Output layer: 1 neuron with Tanh activation\n",
    "\n",
    "We'll use Mean Squared Error (MSE) loss and Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network([\n",
    "    Dense(2, 4),\n",
    "    Tanh(),\n",
    "    Dense(4, 1),\n",
    "    Tanh()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bfec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = MSE()\n",
    "optimizer = SGD(learning_rate=0.5)\n",
    "\n",
    "start_custom = time.time()\n",
    "model.train(X, y, loss_fn, optimizer, epochs=200, verbose=True)\n",
    "custom_time = time.time() - start_custom\n",
    "print(f\"Custom NN training time: {custom_time:.4f} seconds\")\n",
    "model.save_weights('weights/xor_model_weights.npz')\n",
    "plot_losses(model.loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d24182",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.load_weights('weights/xor_model_weights.npz')\n",
    "model.print_final_predictions(X, y)\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9193c417",
   "metadata": {},
   "source": [
    "## Part 2: Gradient Checking\n",
    "\n",
    "We implement a gradient checking function to validate that our backpropagation implementation is correct. This uses numerical differentiation (finite differences) to compare against our analytical gradients for all weight and bias parameters in Dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(model, X, y, loss_fn, epsilon=1e-5, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Performs gradient checking for all Dense layers in the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model    : Network object\n",
    "        X, y     : Input and target output\n",
    "        loss_fn  : Loss class (e.g., MSE)\n",
    "        epsilon  : Small value for numerical gradient\n",
    "        tol      : Tolerance for max difference\n",
    "    \"\"\"\n",
    "    # 1. Forward and backward pass to compute analytical gradients\n",
    "    y_pred = model.forward(X)\n",
    "    grad_output = loss_fn.grad(y_pred, y)\n",
    "    model.backward(grad_output)\n",
    "    \n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'W'):\n",
    "            print(f\"\\n--- Layer {idx} ---\")\n",
    "            \n",
    "            # ----- Gradient for weights -----\n",
    "            num_grad_W = np.zeros_like(layer.W)\n",
    "            for i in range(layer.W.shape[0]):\n",
    "                for j in range(layer.W.shape[1]):\n",
    "                    old_val = layer.W[i,j]\n",
    "                    \n",
    "                    layer.W[i,j] = old_val + epsilon\n",
    "                    loss_plus = loss_fn.loss(model.forward(X), y)\n",
    "                    \n",
    "                    layer.W[i,j] = old_val - epsilon\n",
    "                    loss_minus = loss_fn.loss(model.forward(X), y)\n",
    "                    \n",
    "                    num_grad_W[i,j] = (loss_plus - loss_minus) / (2*epsilon)\n",
    "                    layer.W[i,j] = old_val  # reset\n",
    "            \n",
    "            max_diff_W = np.max(np.abs(num_grad_W - layer.dW))\n",
    "            print(f\"Max difference in W: {max_diff_W}\")\n",
    "            \n",
    "            # ----- Gradient for biases -----\n",
    "            num_grad_b = np.zeros_like(layer.b)\n",
    "            for i in range(layer.b.shape[1]):\n",
    "                old_val = layer.b[0,i]\n",
    "                \n",
    "                layer.b[0,i] = old_val + epsilon\n",
    "                loss_plus = loss_fn.loss(model.forward(X), y)\n",
    "                \n",
    "                layer.b[0,i] = old_val - epsilon\n",
    "                loss_minus = loss_fn.loss(model.forward(X), y)\n",
    "                \n",
    "                num_grad_b[0,i] = (loss_plus - loss_minus) / (2*epsilon)\n",
    "                layer.b[0,i] = old_val  # reset\n",
    "            \n",
    "            max_diff_b = np.max(np.abs(num_grad_b - layer.db))\n",
    "            print(f\"Max difference in b: {max_diff_b}\")\n",
    "            \n",
    "            if max_diff_W < tol and max_diff_b < tol:\n",
    "                print(\"Gradients match within tolerance.\")\n",
    "            else:\n",
    "                print(\"Warning: Gradients may be incorrect.\")\n",
    "\n",
    "gradient_check(model, X, y, loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43866ca8",
   "metadata": {},
   "source": [
    "## Part 3: Comparing results with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9fc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. TensorFlow model training ---\n",
    "tf_model = keras.Sequential([\n",
    "    keras.Input(shape=(2,)),       \n",
    "    keras.layers.Dense(4, activation='tanh'),\n",
    "    keras.layers.Dense(1, activation='tanh')\n",
    "])\n",
    "tf_model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.5),\n",
    "                 loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tf = time.time()\n",
    "tf_model.fit(X, y, epochs=200, verbose=0)\n",
    "tf_model.save_weights('xor_model_tf_weights.weights.h5')\n",
    "tf_time = time.time() - start_tf\n",
    "print(f\"TensorFlow training time: {tf_time:.4f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff92b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('xor_model_weights.npz')\n",
    "y_pred = model.forward(X)\n",
    "final_custom_loss = loss_fn.loss(y_pred, y)\n",
    "\n",
    "tf_model.load_weights('xor_model_tf_weights.weights.h5')\n",
    "y_tf_pred = tf_model.predict(X)\n",
    "final_tf_loss = tf_model.evaluate(X, y, verbose=0)\n",
    "\n",
    "# --- 3. Compare predictions ---\n",
    "diff = np.abs(y_pred - y_tf_pred)\n",
    "max_diff = np.max(diff)\n",
    "\n",
    "# --- 4. Print results ---\n",
    "print(\"\\nCustom model predictions:\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]}, Predicted: {y_pred[i][0]:.4f}, True: {y[i][0]}\")\n",
    "\n",
    "print(\"\\nTensorFlow model predictions:\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]}, Predicted: {y_tf_pred[i][0]:.4f}, True: {y[i][0]}\")\n",
    "\n",
    "print(f\"\\nMax difference between custom model and TensorFlow predictions: {max_diff:.6f}\")\n",
    "print(f\"Custom model training time: {custom_time:.4f} sec\")\n",
    "print(f\"Final custom model loss: {final_custom_loss:.6f}\")\n",
    "print(f\"Final TensorFlow model loss: {final_tf_loss:.6f}\")\n",
    "\n",
    "# --- 5. Plot decision boundary for TensorFlow model ---\n",
    "plot_decision_boundary(lambda x: tf_model(x).numpy(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac5a7c",
   "metadata": {},
   "source": [
    "### **Model Comparison**\n",
    "\n",
    "| Aspect                | Custom Model       | TensorFlow Model       |\n",
    "|-----------------------|-----------------|----------------------|\n",
    "| **Training Time**      | Less | Much more    |\n",
    "| **Ease of Implementation** | Harder        | Easier            |\n",
    "| **Flexibility**        | High           | Medium             |\n",
    "| **Debuggability**      | Easier        | Slightly complex   |\n",
    "| **Integration**        | Manual         | Built-in functions|\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **General Conclusions**\n",
    "\n",
    "1. **Custom model:**  \n",
    "   - Full control over architecture and training.  \n",
    "   - Easier to debug and understand learning behavior.  \n",
    "   - Requires more manual coding and careful tuning.  \n",
    "\n",
    "2. **TensorFlow model:**  \n",
    "   - Fast to implement for standard tasks.  \n",
    "   - Optimized performance with built-in layers and GPU support.  \n",
    "   - Less internal control, but easier to scale.\n",
    "\n",
    "---\n",
    "\n",
    "### **Effect of Changing Parameters**\n",
    "\n",
    "| Parameter       | Effect on Model Behavior                                           |\n",
    "|-----------------|------------------------------------------------------------------|\n",
    "| **Learning Rate** | Higher → faster convergence but risk of overshooting; Lower → slower but stable |\n",
    "| **Epochs**        | More → better convergence but may overfit; Fewer → faster but undertrained |\n",
    "| **Activation Functions** | `tanh` smooths, `relu` faster training, `sigmoid` saturates easily |\n",
    "| **Hidden Units / Layers** | More → captures complex patterns but risk overfitting; Fewer → simpler and faster |\n",
    "\n",
    "**Summary:**  \n",
    "\n",
    "- **Custom model**: Great for learning and experimentation.  \n",
    "- **TensorFlow model**: Ideal for rapid prototyping and scaling.  \n",
    "- Hyperparameters (learning rate, epochs, architecture) directly affect convergence and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8050b",
   "metadata": {},
   "source": [
    "## Milestone 2: Autoencoder on MNIST\n",
    "\n",
    "In this section, we will train an **Autoencoder** to compress handwritten digits (MNIST dataset) into a lower-dimensional latent space and then reconstruct them.\n",
    "\n",
    "**Architecture:**\n",
    "* **Input:** 784 features (flattened 28x28 images)\n",
    "* **Encoder:** Compress to 32 latent features (ReLU activation)\n",
    "* **Decoder:** Reconstruct to 784 features (Sigmoid activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Autoencoder on MNIST Dataset ---\n",
    "# --- Step 1: Data Loading ---\n",
    "(x_train_raw, y_train), (x_test_raw, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# 2. Preprocessing\n",
    "# Flatten images: (N, 28, 28) -> (N, 784)\n",
    "X_train = x_train_raw.reshape(x_train_raw.shape[0], -1)\n",
    "X_test = x_test_raw.reshape(x_test_raw.shape[0], -1)\n",
    "\n",
    "# Normalize to [0, 1] range (required for Sigmoid output match)\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "# 3. Subsampling for quicker demo\n",
    "N_SAMPLES = 2000 \n",
    "N_TEST = 500\n",
    "\n",
    "X_train_small = X_train[:N_SAMPLES]\n",
    "y_train_small = y_train[:N_SAMPLES]\n",
    "X_test_small = X_test[:N_TEST]\n",
    "y_test_small = y_test[:N_TEST]\n",
    "\n",
    "print(f\"Training Data Shape: {X_train_small.shape}\")\n",
    "print(f\"Testing Data Shape: {X_test_small.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969252d",
   "metadata": {},
   "source": [
    "### Training the Autoencoder\n",
    "\n",
    "We define the network using our custom `Network` class. Note that for an autoencoder, the target labels `y` are the same as the input data `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594462d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Architecture\n",
    "input_dim = 784   # 28x28 pixels\n",
    "latent_dim = 64   # Bottleneck layer size\n",
    "\n",
    "layers = [\n",
    "    # Encoder\n",
    "    Dense(input_dim, latent_dim),\n",
    "    ReLU(),\n",
    "    # Decoder\n",
    "    Dense(latent_dim, input_dim),\n",
    "    Sigmoid()  \n",
    "]\n",
    "# 2. Initialize Network, Loss, and Optimizer\n",
    "autoencoder = Network(layers)\n",
    "loss_fn = MSE()\n",
    "optimizer = SGD(learning_rate=0.1) \n",
    "\n",
    "# 3. Train the Network\n",
    "print(\"Starting Autoencoder Training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Note: the target 'y' is the same as input 'X'\n",
    "autoencoder.train(\n",
    "    X_train_small, \n",
    "    X_train_small,  \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    epochs=1000,    \n",
    "    verbose=True\n",
    ")\n",
    "print(f\"Training finished in {(time.time() - start_time)/60:.2f} minutes.\")\n",
    "autoencoder.summary()\n",
    "autoencoder.save_weights('autoencoder_weights.npz')\n",
    "# 4. Visualize Training Loss\n",
    "plot_losses(autoencoder.loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52ef84",
   "metadata": {},
   "source": [
    "### Reconstruction Results\n",
    "\n",
    "We visualize how well the network reconstructs unseen test images. The top row shows the original digits, and the bottom row shows the output from our custom autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b8cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstruction(model, X_original, n_samples=5):\n",
    "    # Get model predictions (reconstructions)\n",
    "    reconstructed = model.forward(X_original[:n_samples])\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(n_samples):\n",
    "        # Plot Original\n",
    "        ax = plt.subplot(2, n_samples, i + 1)\n",
    "        plt.imshow(X_original[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Plot Reconstructed\n",
    "        ax = plt.subplot(2, n_samples, i + 1 + n_samples)\n",
    "        plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Reconstruction results on Test Data:\")\n",
    "autoencoder.load_weights('autoencoder_weights.npz')\n",
    "visualize_reconstruction(autoencoder, X_test_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057ad72",
   "metadata": {},
   "source": [
    "## Part 3: Latent Space Classification\n",
    "\n",
    "Here we demonstrate **Transfer Learning**. We extract the encoder (the first half of the network) to generate compressed 64-dimensional features. We then train a standard SVM classifier on these features to classify the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03eaf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract the Encoder\n",
    "encoder = Network(autoencoder.layers[:2])\n",
    "\n",
    "print(\"Extracting features...\")\n",
    "# Generate latent representations (64-dim vectors) for training and test sets\n",
    "latent_train = encoder.forward(X_train_small)\n",
    "latent_test = encoder.forward(X_test_small)\n",
    "\n",
    "print(f\"Latent Representation Shape: {latent_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train SVM Classifier\n",
    "print(\"Training SVM on latent features...\")\n",
    "svm = SVC(kernel='rbf', C=1.0)\n",
    "svm.fit(latent_train, y_train_small)\n",
    "# 3. Evaluate Performance\n",
    "y_pred = svm.predict(latent_test)\n",
    "acc = accuracy_score(y_test_small, y_pred)\n",
    "\n",
    "print(f\"\\nSVM Classification Accuracy: {acc * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_small, y_pred))\n",
    "# 4. Confusion Matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(confusion_matrix(y_test_small, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix: SVM on Latent Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a58d7c",
   "metadata": {},
   "source": [
    "## Part 4: TensorFlow & Keras Comparison\n",
    "\n",
    "In this final section, we implement the **exact same autoencoder architecture** using TensorFlow/Keras. This serves as a benchmark to validate our custom library's:\n",
    "1.  **Correctness:** Do both models converge to a similar loss?\n",
    "2.  **Performance:** How much faster is the optimized C++ backend of TensorFlow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3de978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section 5: TensorFlow Comparison ---\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# 1. Define the exact same architecture using Keras\n",
    "# Input (784) -> Encoder (64, ReLU) -> Decoder (784, Sigmoid)\n",
    "tf_model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),      # Encoder\n",
    "    layers.Dense(784, activation='sigmoid')   # Decoder\n",
    "])\n",
    "\n",
    "# 2. Compile with same Optimizer and Loss\n",
    "# Note: specific learning rate to match your custom SGD\n",
    "sgd = optimizers.SGD(learning_rate=0.1) \n",
    "tf_model.compile(optimizer=sgd, loss='mse')\n",
    "\n",
    "# 3. Train and Measure Time\n",
    "print(\"Training TensorFlow Model...\")\n",
    "start_time_tf = time.time()\n",
    "\n",
    "# We use batch_size=len(X_train_small) to simulate Full-Batch Gradient Descent \n",
    "history = tf_model.fit(\n",
    "    X_train_small, \n",
    "    X_train_small, \n",
    "    epochs=1000, \n",
    "    batch_size=32, \n",
    "    verbose=True\n",
    ")\n",
    "tf_model.save_weights('autoencoder_model_tf_weights.weights.h5')\n",
    "tf_duration = time.time() - start_time_tf\n",
    "tf_final_loss = history.history['loss'][-1]\n",
    "\n",
    "print(f\"TF Training finished in {tf_duration:.2f} seconds.\")\n",
    "print(f\"TF Final Loss: {tf_final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e6d9c",
   "metadata": {},
   "source": [
    "### Comparison Results\n",
    "\n",
    "We compare the reconstruction quality visually and the training metrics numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_reconstructions(custom_model, tf_model, X, n=5):\n",
    "    # Get predictions\n",
    "    custom_preds = custom_model.forward(X[:n])\n",
    "    tf_preds = tf_model.predict(X[:n], verbose=0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(n):\n",
    "        # 1. Original\n",
    "        ax = plt.subplot(3, n, i + 1)\n",
    "        plt.imshow(X[i].reshape(28, 28), cmap='gray')\n",
    "        if i == 0: ax.set_ylabel(\"Original\", fontsize=12, fontweight='bold')\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # 2. Our Library\n",
    "        ax = plt.subplot(3, n, i + 1 + n)\n",
    "        plt.imshow(custom_preds[i].reshape(28, 28), cmap='gray')\n",
    "        if i == 0: ax.set_ylabel(\"My Library\", fontsize=12, fontweight='bold', color='blue')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # 3. TensorFlow\n",
    "        ax = plt.subplot(3, n, i + 1 + 2*n)\n",
    "        plt.imshow(tf_preds[i].reshape(28, 28), cmap='gray')\n",
    "        if i == 0: ax.set_ylabel(\"TensorFlow\", fontsize=12, fontweight='bold', color='red')\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visual Comparison of Reconstructions:\")\n",
    "autoencoder.load_weights('autoencoder_weights.npz')\n",
    "tf_model.load_weights('autoencoder_model_tf_weights.weights.h5')\n",
    "compare_reconstructions(autoencoder, tf_model, X_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Metrics Comparison Table ---\n",
    "# 1. Retrieve Data\n",
    "# Get the final loss from your custom autoencoder's history\n",
    "custom_loss = autoencoder.loss_history[-1]\n",
    "\n",
    "# 2. Create the Data Dictionary\n",
    "data = {\n",
    "    \"Metric\": [\"Final Loss (MSE)\", \"Training Time (seconds)\"],\n",
    "    \"lib\": [f\"{custom_loss:.6f}\", f\"{time.time() - start_time:.2f}\"], \n",
    "    \"TensorFlow / Keras\": [f\"{tf_final_loss:.6f}\", f\"{tf_duration:.2f}\"]\n",
    "}\n",
    "\n",
    "# 3. Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 4. Display as a styled table\n",
    "print(\"\\n\" + \"=\"*20 + \" FINAL BENCHMARK RESULTS \" + \"=\"*20)\n",
    "display(df.style.hide(axis='index'))\n",
    "\n",
    "# 5. Print formatted Markdown for your PDF report\n",
    "print(\"\\n[Copy-Paste for Report] Markdown Format:\")\n",
    "markdown_table = df.to_markdown(index=False)\n",
    "print(markdown_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
>>>>>>> Stashed changes
